{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Abstract \"Translation\"\n",
    "In the original project, I trained a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French. This notebook was modified such that abstracts became the English text and the titles became the French text. The abstracts are defined as the source_text and the titles are defined as the target_text\n",
    "## Get the Data\n",
    "The data comes from the discovery set made by Roxana and Diego. The abstracts and titles were further cleaned as shown in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('clean_abstracts_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = data.abstracts #helper.load_data(source_path)\n",
    "titles = data.titles #helper.load_data(target_path)\n",
    "\n",
    "with open('titles.txt', 'w+') as file:\n",
    "    for title in titles:\n",
    "        file.write(title+'\\n')\n",
    "    file.close()\n",
    "    \n",
    "with open('abstracts.txt', 'w+') as file:\n",
    "    for abstract in abstracts:\n",
    "        file.write(abstract+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "source_path = 'abstracts.txt'\n",
    "target_path = 'titles.txt'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Feel free to play around with view_sentence_range to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 25685\n",
      "Number of abstracts: 4965\n",
      "Average number of words in an abstract: 201.70231621349447\n",
      "\n",
      "Abstracts 0 to 10:\n",
      "we present an ultra deep survey for neptune trojans using the subaru m and magellan m telescopes the survey reached a detection efficiency in the r band at magnitudes and covered square degrees of sky this depth corresponds to neptune trojans that are about km in radius assuming an albedo of a paucity of smaller neptune trojans radii km compared to larger ones was found the brightest neptune trojans appear to follow a steep power law slope q similar to the brightest objects in the other known stable reservoirs such as the kuiper belt, jupiter trojans and main belt asteroids we find a roll over for the neptune trojans that occurs around a radii of r km mags, which is also very similar to the other stable reservoirs all the observed stable regions in the the solar system show evidence for missing intermediate sized planetesimals misps this indicates a primordial and not collisional origin, which suggests planetesimal formation proceeded directly from small to large objects the scarcity of intermediate and smaller sized neptune trojans may limit them as being a strong source for the short period comets\n",
      "significant proton fluxes were detected in the near wake region of the moon by an ion mass spectrometer on board chandrayaan the energy of these nightside protons is slightly higher than the energy of the solar wind protons the protons are detected close to the lunar equatorial plane at a circ solar zenith angle, ie, circ behind the terminator at a height of km the protons come from just above the local horizon, and move along the magnetic field in the solar wind reference frame we compared the observed proton flux with the predictions from analytical models of an electrostatic plasma expansion into a vacuum the observed velocity was higher than the velocity predicted by analytical models by a factor of to the simple analytical models cannot explain the observed ion dynamics along the magnetic field in the vicinity of the moon\n",
      "we study the role of ambipolar diffusion ad on the non linear evolution of the mri in protoplanetary disks using the strong coupling limit, which applies when the electron recombination time is much shorter than the orbital time the effect of ad in this limit is characterized by the dimensionless number am, the frequency of which neutral particles collide with ions normalized to the orbital frequency we perform three dimensional unstratified shearing box simulations of the mri over a wide range of am as well as different magnetic field strengths and geometries the saturation level of the mri turbulence depends on the magnetic geometry and increases with the net magnetic flux there is an upper limit to the net flux for sustained turbulence, corresponding to the requirement that the most unstable vertical wavelength be less than the disk scale height correspondingly, at a given am, there exists a maximum value of the turbulent stress alphamax for am, the largest stress is associated with a field geometry that has both net vertical and toroidal flux in this case, we confirm the results of linear analyses that show the fastest growing mode has a non zero radial wave number with growth rate exceeding the pure vertical field case we find there is a very tight correlation between the turbulent stress alpha and the plasma betapgaspmagalpha at the saturated state of the mri turbulence regardless of field geometry, and alphamax rapidly decreases with decreasing am in particular, we quote alphamax for am and alphamax for am\n",
      "measurements of the interplanetary magnetic field imf over several solar cycles do not agree with computed values of open magnetic flux from potential field extrapolations the discrepancy becomes greater around solar maximum in each cycle, when the imf can be twice as strong as predicted by the potential field model here we demonstrate that this discrepancy may be resolved by allowing for electric currents in the low corona below solar radii we present a quasi static numerical model of the large scale coronal magnetic evolution, which systematically produces these currents through flux emergence and shearing by surface motions the open flux is increased by at solar maximum, but only at solar minimum, bringing it in line with estimates from imf measurements the additional open flux in the non potential model arises through inflation of the magnetic field by electric currents, with super imposed fluctuations due to coronal mass ejections the latter are modelled by the self consistent ejection of twisted magnetic flux ropes\n",
      "we have designed an inquiry based laboratory activity on transiting extrasolar planets for an introductory college level astronomy class the activity was designed with the intent of simultaneously teaching science process skills and factual content about transits and light curves in the activity, groups of two to four students each formulate a specific science question and design and carry out an investigation using a table top model of a star and orbiting planet each group then presents their findings to other students in their class in a final presentation, the instructors integrate students findings with a summary of how measured light curves indicate properties of planetary systems the activity debuted at hartnell college in november and has also been adapted for a lecture based astronomy course at uc santa cruz we present the results of student surveys before and after the astronomy course at hartnell and discuss how well our activity promotes students confidence and identity as scientists, relative to traditional lab activities\n",
      "several photometric surveys for short period transiting giant planets have targeted a number of open clusters, but no convincing detections have been made although each individual survey typically targeted an insufficient number of stars to expect a detection assuming the frequency of short period giant planets found in surveys of field stars, we ask whether the lack of detections from the ensemble of open cluster surveys is inconsistent with expectations from the field planet population we select a subset of existing transit surveys with well defined selection criteria and quantified detection efficiencies, and statistically combine their null results to show that the upper limit on the planet fraction is and for rj and rj planets, respectively in the p day period range for the period range of p days we find upper limits of and for rj and rj, respectively comparing these results to the frequency of short period giant planets around field stars in both radial velocity and transit surveys, we conclude that there is no evidence to suggest that open clusters support a fundamentally different planet population than field stars given the available data\n",
      "as an alternative to either directly assimilating radiances or the naive use of retrieved profiles of temperature, humidity, aerosols, and chemical species, a strategy is described that makes use of the so called averaging kernel ak and other information from the retrieval process this ak approach has the potential to improve the use of remotely sensed observations of the atmosphere first, we show how to use the ak and the retrieval noise covariance to transform the retrieved quantities into observations that are unbiased and have uncorrelated errors, and to eliminate both the smoothing inherent in the retrieval process and the effect of the prior since the effect of the prior is removed, any prior, including the forecast from the data assimilation cycle can be used then we show how to transform this result into eof space, when a truncated eof series has been used in the retrieval process this provides a degree of data compression and eliminates those transformed variables that have very small information content in both approaches a vertical interpolation from the dynamical model coordinate to the radiative transfer coordinate is required we define an algorithm using the eof representation to optimize this vertical interpolation\n",
      "correlations between stellar properties and the occurrence rate of exoplanets can be used to inform the target selection of future planet search efforts and provide valuable clues about the planet formation process we analyze a sample of stars drawn from the california planet survey targets to determine the empirical functional form describing the likelihood of a star harboring a giant planet as a function of its mass and metallicity our stellar sample ranges from m dwarfs with masses as low as msun to intermediate mass subgiants with masses as high as msun in agreement with previous studies, our sample exhibits a planet metallicity correlation at all stellar masses the fraction of stars that harbor giant planets scales as f propto feh we can rule out a flat metallicity relationship among our evolved stars at confidence, which argues that the high metallicities of stars with planets are not likely due to convective envelope pollution our data also rule out a constant planet occurrence rate for feh , indicating that giant planets continue to become rarer at sub solar metallicities we also find that planet occurrence increases with stellar mass f propto mstar, characterized by a rise from around m dwarfs msun to around a stars msun, at solar metallicity we argue that the correlation between stellar properties and giant planet occurrence is strong supporting evidence of the core accretion model of planet formation\n",
      "among other things, studies of the formation and evolution of planetary systems currently draw on two important observational resources the precise characterization available for planets that transit their parent stars and the frequency and nature of systems with multiple planets thus far, the study of transiting exoplanets has focused almost exclusively on systems with only one planet, except for considering the influence of additional planets on the transit light curve, mostly through transit timing variations ttvs this work considers systems where multiple planets are seen to transit the same star and concludes that such multi transiting systems will be the most information rich planetary systems besides our own solar system five new candidate multi transiting systems from emphkepler have been announced in steffen et al , though these candidates have not yet been fully confirmed as planets in anticipation of the likely confirmation of multi transiting systems, we discuss the value of these systems in detail for example, proper interpretation of transit timing variations is significantly improved in multi transiting systems the true mutual inclination, a valuable probe of planetary formation, can also be well determined in certain systems, especially through rossiter mclaughlin measurements of each planet in addition, such systems may undergo predictable and observable mutual events, where one planet crosses over the other, which allow for unique constraints on various physical and orbital parameters, particularly the mutual inclination\n",
      "we discuss one important aspect of waldmeier effect which says that the stronger cycles rise rapidly than weaker cycles we studied four different data set of solar activity indices, and find strong linear correlation between rise rates and amplitudes of solar activity we study this effect theoretically by introducing suitable stochastic fluctuations in our regular solar dynamo model\n",
      "\n",
      "Titles 0 to 10:\n",
      "the size distribution of the neptune trojans and the missing intermediate sized planetesimals\n",
      "protons in the near lunar wake observed by the sub kev atom reflection analyzer on board chandrayaan\n",
      "effect of ambipolar diffusion on the non linear evolution of magnetorotational instability in weakly ionized disks\n",
      "a non potential model for the suns open magnetic flux\n",
      "a college level inquiry based laboratory activity on transiting planets\n",
      "ensemble analysis of open cluster transit surveys upper limits on the frequency of short period planets consistent with the field\n",
      "a retrieval strategy for interactive ensemble data assimilation\n",
      "giant planet occurrence in the stellar mass metallicity plane\n",
      "the value of systems with multiple transiting planets\n",
      "correlation between the rise rate and the amplitude of the solar magnetic cycles\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of abstracts: {}'.format(len(sentences)))\n",
    "print('Average number of words in an abstract: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('Abstracts {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('Titles {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Function\n",
    "### Text to Word Ids\n",
    "As you did with other RNNs, you must turn the text into a number so the computer can understand it. In the function `text_to_ids()`, you'll turn `source_text` and `target_text` from words to ids.  However, you need to add the `<EOS>` word id at the end of `target_text`.  This will help the neural network predict when the sentence should end.\n",
    "\n",
    "You can get the `<EOS>` word id by doing:\n",
    "```python\n",
    "target_vocab_to_int['<EOS>']\n",
    "```\n",
    "You can get other word ids using `source_vocab_to_int` and `target_vocab_to_int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    source_id_text = [[source_vocab_to_int[word] for word in sentence.split()] \\\n",
    "                      for sentence in source_text.split('\\n')] \n",
    "    target_id_text = [[target_vocab_to_int[word] for word in sentence.split()] + [target_vocab_to_int['<EOS>']] \\\n",
    "                      for sentence in target_text.split('\\n')]\n",
    "    return source_id_text, target_id_text\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "This will check to make sure you have the correct version of TensorFlow and access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:\n",
    "- `model_inputs`\n",
    "- `process_decoder_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### Input\n",
    "Implement the `model_inputs()` function to create TF Placeholders for the Neural Network. It should create the following placeholders:\n",
    "\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "- Targets placeholder with rank 2.\n",
    "- Learning rate placeholder with rank 0.\n",
    "- Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "- Target sequence length placeholder named \"target_sequence_length\" with rank 1\n",
    "- Max target sequence length tensor named \"max_target_len\" getting its value from applying tf.reduce_max on the target_sequence_length placeholder. Rank 0.\n",
    "- Source sequence length placeholder named \"source_sequence_length\" with rank 1\n",
    "\n",
    "Return the placeholders in the following the tuple (input, targets, learning rate, keep probability, target sequence length, max target sequence length, source sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32, [None, None])\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    \n",
    "    max_target_len = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    \n",
    "    source_sequence_length = tf.placeholder(tf.int32, [None],name='source_sequence_length')\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_prob, target_sequence_length, max_target_len, source_sequence_length\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "Implement `process_decoder_input` by removing the last word id from each batch in `target_data` and concat the GO ID to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    \n",
    "    proc_input = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return proc_input\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_process_encoding_input(process_decoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Implement `encoding_layer()` to create a Encoder RNN layer:\n",
    " * Embed the encoder input using [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n",
    " * Construct a [stacked](https://github.com/tensorflow/tensorflow/blob/6947f65a374ebf29e74bb71e36fd82760056d82c/tensorflow/docs_src/tutorials/recurrent.md#stacking-multiple-lstms) [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) wrapped in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)\n",
    " * Pass cell and embedded input to [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "reload(tests)\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
    "    :param source_vocab_size: vocabulary size of source data\n",
    "    :param encoding_embedding_size: embedding size of source data\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embedded_input = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                                      source_vocab_size, \n",
    "                                                      encoding_embedding_size)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size) \\\n",
    "                                        for _ in range(num_layers)])\n",
    "    \n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    output, state = tf.nn.dynamic_rnn(cell, embedded_input, \n",
    "                                      sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return output, state\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_encoding_layer(encoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Create a training decoding layer:\n",
    "* Create a [`tf.contrib.seq2seq.TrainingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper) \n",
    "* Create a [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "* Obtain the decoder outputs from [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_summary_length: The length of the longest sequence in the batch\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    ##1. Apply TrainingHelper\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                    sequence_length=target_sequence_length,\n",
    "                                                    time_major=False)\n",
    "    \n",
    "    ##2. Apply BasicDecoder\n",
    "    train_decode = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                 train_helper,\n",
    "                                                 encoder_state,\n",
    "                                                 output_layer)\n",
    "    \n",
    "    ##3. Apply dynamic_decode\n",
    "    train_decode_output, _ = tf.contrib.seq2seq.dynamic_decode(train_decode,\n",
    "                                                              impute_finished=True,\n",
    "                                                              maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return train_decode_output\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Create inference decoder:\n",
    "* Create a [`tf.contrib.seq2seq.GreedyEmbeddingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper)\n",
    "* Create a [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "* Obtain the decoder outputs from [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param vocab_size: Size of decoder/target vocabulary\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param batch_size: Batch size\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    ##O. Create start tokens\n",
    "    start_tokens = tf.tile(tf.constant([start_of_sequence_id], dtype=tf.int32),\n",
    "                          [batch_size], name='start_tokens')\n",
    "    \n",
    "    ##1. Apply GreedyEmbeddingHelper\n",
    "    infer_help = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                         start_tokens,\n",
    "                                                         end_of_sequence_id)\n",
    "    \n",
    "    ##2. Apply BasicDecoder\n",
    "    infer_decode = tf.contrib.seq2seq.BasicDecoder(dec_cell, infer_help,\n",
    "                                                  encoder_state, output_layer)\n",
    "    \n",
    "    ##3. Apply dynamic_decode\n",
    "    infer_decode_output, _ = tf.contrib.seq2seq.dynamic_decode(infer_decode,\n",
    "                                                              impute_finished=True,\n",
    "                                                              maximum_iterations=max_target_sequence_length)\n",
    "    \n",
    "    return infer_decode_output\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_infer(decoding_layer_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoding Layer\n",
    "Implement `decoding_layer()` to create a Decoder RNN layer.\n",
    "\n",
    "* Embed the target sequences\n",
    "* Construct the decoder LSTM cell (just like you constructed the encoder cell above)\n",
    "* Create an output layer to map the outputs of the decoder to the elements of our vocabulary\n",
    "* Use the your `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)` function to get the training logits.\n",
    "* Use your `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, max_target_sequence_length, vocab_size, output_layer, batch_size, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: You'll need to use [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_input: Decoder input\n",
    "    :param encoder_state: Encoder state\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param target_vocab_size: Size of target vocabulary\n",
    "    :param batch_size: The size of the batch\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param decoding_embedding_size: Decoding embedding size\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    ##1. Embedded the target sequence\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    \n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    ##2. Construct LSTM Cell\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size) \\\n",
    "                                            for _ in range(num_layers)])\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    ##3. Create mapping outpult layer\n",
    "    output_layer = Dense(target_vocab_size, \n",
    "                       kernel_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    ##4. Do the decoding for training and inference\n",
    "    with tf.variable_scope('decode'):\n",
    "        train_decode_output = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                                                  target_sequence_length, max_target_sequence_length, \n",
    "                                                  output_layer, keep_prob)\n",
    "        \n",
    "    with tf.variable_scope('decode', reuse=True):\n",
    "        infer_decode_output = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, \n",
    "                                                  target_vocab_to_int['<GO>'], \n",
    "                                                  target_vocab_to_int['<EOS>'], \n",
    "                                                  max_target_sequence_length, target_vocab_size, output_layer, \n",
    "                                                  batch_size, keep_prob)\n",
    "    \n",
    "    return train_decode_output, infer_decode_output\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "\n",
    "- Encode the input using your `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,  source_sequence_length, source_vocab_size, encoding_embedding_size)`.\n",
    "- Process target data using your `process_decoder_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Decode the encoded input using your `decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param source_sequence_length: Sequence Lengths of source sequences in the batch\n",
    "    :param target_sequence_length: Sequence Lengths of target sequences in the batch\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    ##1. Encode input using enconding_layer()\n",
    "    _, enc_input = encoding_layer(input_data, rnn_size, num_layers, keep_prob,  \n",
    "                                   source_sequence_length, source_vocab_size, \n",
    "                                   enc_embedding_size)\n",
    "    \n",
    "    ##2. Process target data with procesed_decoder_input()\n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    ##3. Decode input using decoding_layer\n",
    "    train_output, infer_output = decoding_layer(dec_input, enc_input, target_sequence_length, \n",
    "                                                max_target_sentence_length, rnn_size, num_layers, \n",
    "                                                target_vocab_to_int, target_vocab_size, batch_size, \n",
    "                                                keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_seq2seq_model(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `num_layers` to the number of layers.\n",
    "- Set `encoding_embedding_size` to the size of the embedding for the encoder.\n",
    "- Set `decoding_embedding_size` to the size of the embedding for the decoder.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `keep_probability` to the Dropout keep probability\n",
    "- Set `display_step` to state how many steps between each debug output statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 12\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "# RNN Size\n",
    "rnn_size = 300\n",
    "# Number of Layers\n",
    "num_layers = 3\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 130\n",
    "decoding_embedding_size = 130\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.8\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "\n",
    "    #sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch and pad the source and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   10/155 - Train Accuracy: 0.4672, Validation Accuracy: 0.4107, Loss: 4.6685\n",
      "Epoch   0 Batch   20/155 - Train Accuracy: 0.4243, Validation Accuracy: 0.4152, Loss: 4.3015\n",
      "Epoch   0 Batch   30/155 - Train Accuracy: 0.5453, Validation Accuracy: 0.4241, Loss: 3.4541\n",
      "Epoch   0 Batch   40/155 - Train Accuracy: 0.3247, Validation Accuracy: 0.4167, Loss: 4.7904\n",
      "Epoch   0 Batch   50/155 - Train Accuracy: 0.3891, Validation Accuracy: 0.4241, Loss: 4.4062\n",
      "Epoch   0 Batch   60/155 - Train Accuracy: 0.3899, Validation Accuracy: 0.4211, Loss: 4.2519\n",
      "Epoch   0 Batch   70/155 - Train Accuracy: 0.4763, Validation Accuracy: 0.4211, Loss: 3.6204\n",
      "Epoch   0 Batch   80/155 - Train Accuracy: 0.5156, Validation Accuracy: 0.4211, Loss: 3.4580\n",
      "Epoch   0 Batch   90/155 - Train Accuracy: 0.5448, Validation Accuracy: 0.4211, Loss: 3.1855\n",
      "Epoch   0 Batch  100/155 - Train Accuracy: 0.5409, Validation Accuracy: 0.4256, Loss: 3.2271\n",
      "Epoch   0 Batch  110/155 - Train Accuracy: 0.5264, Validation Accuracy: 0.4211, Loss: 3.2297\n",
      "Epoch   0 Batch  120/155 - Train Accuracy: 0.5368, Validation Accuracy: 0.4256, Loss: 3.0850\n",
      "Epoch   0 Batch  130/155 - Train Accuracy: 0.4048, Validation Accuracy: 0.4211, Loss: 4.0362\n",
      "Epoch   0 Batch  140/155 - Train Accuracy: 0.4125, Validation Accuracy: 0.4256, Loss: 4.0906\n",
      "Epoch   0 Batch  150/155 - Train Accuracy: 0.4792, Validation Accuracy: 0.4256, Loss: 3.5024\n",
      "Epoch   1 Batch   10/155 - Train Accuracy: 0.4562, Validation Accuracy: 0.4226, Loss: 3.3911\n",
      "Epoch   1 Batch   20/155 - Train Accuracy: 0.4342, Validation Accuracy: 0.4211, Loss: 3.5693\n",
      "Epoch   1 Batch   30/155 - Train Accuracy: 0.5259, Validation Accuracy: 0.4092, Loss: 2.9656\n",
      "Epoch   1 Batch   40/155 - Train Accuracy: 0.2882, Validation Accuracy: 0.3958, Loss: 4.2293\n",
      "Epoch   1 Batch   50/155 - Train Accuracy: 0.2641, Validation Accuracy: 0.3080, Loss: 3.8895\n",
      "Epoch   1 Batch   60/155 - Train Accuracy: 0.3244, Validation Accuracy: 0.3125, Loss: 3.8862\n",
      "Epoch   1 Batch   70/155 - Train Accuracy: 0.2412, Validation Accuracy: 0.0997, Loss: 3.3180\n",
      "Epoch   1 Batch   80/155 - Train Accuracy: 0.0240, Validation Accuracy: 0.0446, Loss: 3.1253\n",
      "Epoch   1 Batch   90/155 - Train Accuracy: 0.0938, Validation Accuracy: 0.0506, Loss: 2.9581\n",
      "Epoch   1 Batch  100/155 - Train Accuracy: 0.0776, Validation Accuracy: 0.0506, Loss: 3.0541\n",
      "Epoch   1 Batch  110/155 - Train Accuracy: 0.0228, Validation Accuracy: 0.0446, Loss: 3.0291\n",
      "Epoch   1 Batch  120/155 - Train Accuracy: 0.0312, Validation Accuracy: 0.0446, Loss: 2.9201\n",
      "Epoch   1 Batch  130/155 - Train Accuracy: 0.0227, Validation Accuracy: 0.0446, Loss: 3.8457\n",
      "Epoch   1 Batch  140/155 - Train Accuracy: 0.0266, Validation Accuracy: 0.0446, Loss: 3.8762\n",
      "Epoch   1 Batch  150/155 - Train Accuracy: 0.0273, Validation Accuracy: 0.0446, Loss: 3.2914\n",
      "Epoch   2 Batch   10/155 - Train Accuracy: 0.0281, Validation Accuracy: 0.0446, Loss: 3.2331\n",
      "Epoch   2 Batch   20/155 - Train Accuracy: 0.0345, Validation Accuracy: 0.0446, Loss: 3.3431\n",
      "Epoch   2 Batch   30/155 - Train Accuracy: 0.0302, Validation Accuracy: 0.0446, Loss: 2.8357\n",
      "Epoch   2 Batch   40/155 - Train Accuracy: 0.0330, Validation Accuracy: 0.0446, Loss: 3.9984\n",
      "Epoch   2 Batch   50/155 - Train Accuracy: 0.0266, Validation Accuracy: 0.0446, Loss: 3.6883\n",
      "Epoch   2 Batch   60/155 - Train Accuracy: 0.0521, Validation Accuracy: 0.0446, Loss: 3.6495\n",
      "Epoch   2 Batch   70/155 - Train Accuracy: 0.0288, Validation Accuracy: 0.0446, Loss: 3.1464\n",
      "Epoch   2 Batch   80/155 - Train Accuracy: 0.0601, Validation Accuracy: 0.0580, Loss: 2.9913\n",
      "Epoch   2 Batch   90/155 - Train Accuracy: 0.1510, Validation Accuracy: 0.0595, Loss: 2.8478\n",
      "Epoch   2 Batch  100/155 - Train Accuracy: 0.1821, Validation Accuracy: 0.0476, Loss: 2.9275\n",
      "Epoch   2 Batch  110/155 - Train Accuracy: 0.1442, Validation Accuracy: 0.0699, Loss: 2.8937\n",
      "Epoch   2 Batch  120/155 - Train Accuracy: 0.1384, Validation Accuracy: 0.0461, Loss: 2.8061\n",
      "Epoch   2 Batch  130/155 - Train Accuracy: 0.1364, Validation Accuracy: 0.1176, Loss: 3.7429\n",
      "Epoch   2 Batch  140/155 - Train Accuracy: 0.0328, Validation Accuracy: 0.0580, Loss: 3.7084\n",
      "Epoch   2 Batch  150/155 - Train Accuracy: 0.1823, Validation Accuracy: 0.0833, Loss: 3.1657\n",
      "Epoch   3 Batch   10/155 - Train Accuracy: 0.1625, Validation Accuracy: 0.1682, Loss: 3.1124\n",
      "Epoch   3 Batch   20/155 - Train Accuracy: 0.0789, Validation Accuracy: 0.1324, Loss: 3.2141\n",
      "Epoch   3 Batch   30/155 - Train Accuracy: 0.0636, Validation Accuracy: 0.0446, Loss: 2.7366\n",
      "Epoch   3 Batch   40/155 - Train Accuracy: 0.2170, Validation Accuracy: 0.3185, Loss: 3.8495\n",
      "Epoch   3 Batch   50/155 - Train Accuracy: 0.0703, Validation Accuracy: 0.1131, Loss: 3.5715\n",
      "Epoch   3 Batch   60/155 - Train Accuracy: 0.2188, Validation Accuracy: 0.2366, Loss: 3.4903\n",
      "Epoch   3 Batch   70/155 - Train Accuracy: 0.3250, Validation Accuracy: 0.2009, Loss: 3.0751\n",
      "Epoch   3 Batch   80/155 - Train Accuracy: 0.4014, Validation Accuracy: 0.2783, Loss: 2.8938\n",
      "Epoch   3 Batch   90/155 - Train Accuracy: 0.4885, Validation Accuracy: 0.2887, Loss: 2.7625\n",
      "Epoch   3 Batch  100/155 - Train Accuracy: 0.4019, Validation Accuracy: 0.2054, Loss: 2.8557\n",
      "Epoch   3 Batch  110/155 - Train Accuracy: 0.4303, Validation Accuracy: 0.2902, Loss: 2.7860\n",
      "Epoch   3 Batch  120/155 - Train Accuracy: 0.4219, Validation Accuracy: 0.1949, Loss: 2.7563\n",
      "Epoch   3 Batch  130/155 - Train Accuracy: 0.3139, Validation Accuracy: 0.3333, Loss: 3.6166\n",
      "Epoch   3 Batch  140/155 - Train Accuracy: 0.2375, Validation Accuracy: 0.2440, Loss: 3.5864\n",
      "Epoch   3 Batch  150/155 - Train Accuracy: 0.3932, Validation Accuracy: 0.3006, Loss: 3.0608\n",
      "Epoch   4 Batch   10/155 - Train Accuracy: 0.2422, Validation Accuracy: 0.2188, Loss: 3.0042\n",
      "Epoch   4 Batch   20/155 - Train Accuracy: 0.2204, Validation Accuracy: 0.2842, Loss: 3.1305\n",
      "Epoch   4 Batch   30/155 - Train Accuracy: 0.4116, Validation Accuracy: 0.2173, Loss: 2.6767\n",
      "Epoch   4 Batch   40/155 - Train Accuracy: 0.3003, Validation Accuracy: 0.4077, Loss: 3.7130\n",
      "Epoch   4 Batch   50/155 - Train Accuracy: 0.3422, Validation Accuracy: 0.3839, Loss: 3.5101\n",
      "Epoch   4 Batch   60/155 - Train Accuracy: 0.3125, Validation Accuracy: 0.3125, Loss: 3.3692\n",
      "Epoch   4 Batch   70/155 - Train Accuracy: 0.4000, Validation Accuracy: 0.2753, Loss: 2.9914\n",
      "Epoch   4 Batch   80/155 - Train Accuracy: 0.4267, Validation Accuracy: 0.2961, Loss: 2.8130\n",
      "Epoch   4 Batch   90/155 - Train Accuracy: 0.4740, Validation Accuracy: 0.2679, Loss: 2.7062\n",
      "Epoch   4 Batch  100/155 - Train Accuracy: 0.4795, Validation Accuracy: 0.2827, Loss: 2.7905\n",
      "Epoch   4 Batch  110/155 - Train Accuracy: 0.5132, Validation Accuracy: 0.3750, Loss: 2.7017\n",
      "Epoch   4 Batch  120/155 - Train Accuracy: 0.5089, Validation Accuracy: 0.2946, Loss: 2.6720\n",
      "Epoch   4 Batch  130/155 - Train Accuracy: 0.3878, Validation Accuracy: 0.3839, Loss: 3.5195\n",
      "Epoch   4 Batch  140/155 - Train Accuracy: 0.3703, Validation Accuracy: 0.3765, Loss: 3.4806\n",
      "Epoch   4 Batch  150/155 - Train Accuracy: 0.3867, Validation Accuracy: 0.3586, Loss: 2.9860\n",
      "Epoch   5 Batch   10/155 - Train Accuracy: 0.3969, Validation Accuracy: 0.3423, Loss: 2.8804\n",
      "Epoch   5 Batch   20/155 - Train Accuracy: 0.3339, Validation Accuracy: 0.3586, Loss: 3.0458\n",
      "Epoch   5 Batch   30/155 - Train Accuracy: 0.5216, Validation Accuracy: 0.3735, Loss: 2.6231\n",
      "Epoch   5 Batch   40/155 - Train Accuracy: 0.2934, Validation Accuracy: 0.3929, Loss: 3.6206\n",
      "Epoch   5 Batch   50/155 - Train Accuracy: 0.2000, Validation Accuracy: 0.2589, Loss: 3.4187\n",
      "Epoch   5 Batch   60/155 - Train Accuracy: 0.3646, Validation Accuracy: 0.3735, Loss: 3.2580\n",
      "Epoch   5 Batch   70/155 - Train Accuracy: 0.4150, Validation Accuracy: 0.3170, Loss: 2.9333\n",
      "Epoch   5 Batch   80/155 - Train Accuracy: 0.4988, Validation Accuracy: 0.3512, Loss: 2.7079\n",
      "Epoch   5 Batch   90/155 - Train Accuracy: 0.5323, Validation Accuracy: 0.3943, Loss: 2.6165\n",
      "Epoch   5 Batch  100/155 - Train Accuracy: 0.5259, Validation Accuracy: 0.3646, Loss: 2.7116\n",
      "Epoch   5 Batch  110/155 - Train Accuracy: 0.5180, Validation Accuracy: 0.3869, Loss: 2.5881\n",
      "Epoch   5 Batch  120/155 - Train Accuracy: 0.5033, Validation Accuracy: 0.3393, Loss: 2.6109\n",
      "Epoch   5 Batch  130/155 - Train Accuracy: 0.3764, Validation Accuracy: 0.3854, Loss: 3.4297\n",
      "Epoch   5 Batch  140/155 - Train Accuracy: 0.3812, Validation Accuracy: 0.3884, Loss: 3.3693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch  150/155 - Train Accuracy: 0.4479, Validation Accuracy: 0.3899, Loss: 2.9070\n",
      "Epoch   6 Batch   10/155 - Train Accuracy: 0.4406, Validation Accuracy: 0.3780, Loss: 2.7608\n",
      "Epoch   6 Batch   20/155 - Train Accuracy: 0.3882, Validation Accuracy: 0.3869, Loss: 2.9814\n",
      "Epoch   6 Batch   30/155 - Train Accuracy: 0.5399, Validation Accuracy: 0.3854, Loss: 2.5542\n",
      "Epoch   6 Batch   40/155 - Train Accuracy: 0.3038, Validation Accuracy: 0.4062, Loss: 3.5264\n",
      "Epoch   6 Batch   50/155 - Train Accuracy: 0.2656, Validation Accuracy: 0.3006, Loss: 3.2799\n",
      "Epoch   6 Batch   60/155 - Train Accuracy: 0.3750, Validation Accuracy: 0.4048, Loss: 3.2079\n",
      "Epoch   6 Batch   70/155 - Train Accuracy: 0.4225, Validation Accuracy: 0.3586, Loss: 2.8502\n",
      "Epoch   6 Batch   80/155 - Train Accuracy: 0.4892, Validation Accuracy: 0.3408, Loss: 2.6342\n",
      "Epoch   6 Batch   90/155 - Train Accuracy: 0.5302, Validation Accuracy: 0.3854, Loss: 2.5798\n",
      "Epoch   6 Batch  100/155 - Train Accuracy: 0.5269, Validation Accuracy: 0.3393, Loss: 2.6114\n",
      "Epoch   6 Batch  110/155 - Train Accuracy: 0.5072, Validation Accuracy: 0.3780, Loss: 2.4991\n",
      "Epoch   6 Batch  120/155 - Train Accuracy: 0.5145, Validation Accuracy: 0.3378, Loss: 2.5171\n",
      "Epoch   6 Batch  130/155 - Train Accuracy: 0.3821, Validation Accuracy: 0.3854, Loss: 3.3530\n",
      "Epoch   6 Batch  140/155 - Train Accuracy: 0.3891, Validation Accuracy: 0.3676, Loss: 3.3604\n",
      "Epoch   6 Batch  150/155 - Train Accuracy: 0.4531, Validation Accuracy: 0.3973, Loss: 2.8473\n",
      "Epoch   7 Batch   10/155 - Train Accuracy: 0.4375, Validation Accuracy: 0.3780, Loss: 2.7082\n",
      "Epoch   7 Batch   20/155 - Train Accuracy: 0.4095, Validation Accuracy: 0.4003, Loss: 2.8449\n",
      "Epoch   7 Batch   30/155 - Train Accuracy: 0.5323, Validation Accuracy: 0.4003, Loss: 2.4760\n",
      "Epoch   7 Batch   40/155 - Train Accuracy: 0.3108, Validation Accuracy: 0.3973, Loss: 3.3875\n",
      "Epoch   7 Batch   50/155 - Train Accuracy: 0.1844, Validation Accuracy: 0.1801, Loss: 3.2066\n",
      "Epoch   7 Batch   60/155 - Train Accuracy: 0.3646, Validation Accuracy: 0.4122, Loss: 3.1663\n",
      "Epoch   7 Batch   70/155 - Train Accuracy: 0.3150, Validation Accuracy: 0.2664, Loss: 2.7951\n",
      "Epoch   7 Batch   80/155 - Train Accuracy: 0.4940, Validation Accuracy: 0.3899, Loss: 2.5902\n",
      "Epoch   7 Batch   90/155 - Train Accuracy: 0.5229, Validation Accuracy: 0.3601, Loss: 2.5032\n",
      "Epoch   7 Batch  100/155 - Train Accuracy: 0.5011, Validation Accuracy: 0.3229, Loss: 2.5663\n",
      "Epoch   7 Batch  110/155 - Train Accuracy: 0.5300, Validation Accuracy: 0.3929, Loss: 2.4482\n",
      "Epoch   7 Batch  120/155 - Train Accuracy: 0.5190, Validation Accuracy: 0.3690, Loss: 2.4598\n",
      "Epoch   7 Batch  130/155 - Train Accuracy: 0.3864, Validation Accuracy: 0.3839, Loss: 3.2639\n",
      "Epoch   7 Batch  140/155 - Train Accuracy: 0.3281, Validation Accuracy: 0.3006, Loss: 3.2185\n",
      "Epoch   7 Batch  150/155 - Train Accuracy: 0.4701, Validation Accuracy: 0.4196, Loss: 2.8394\n",
      "Epoch   8 Batch   10/155 - Train Accuracy: 0.4641, Validation Accuracy: 0.3929, Loss: 2.6081\n",
      "Epoch   8 Batch   20/155 - Train Accuracy: 0.3980, Validation Accuracy: 0.3854, Loss: 2.7419\n",
      "Epoch   8 Batch   30/155 - Train Accuracy: 0.5377, Validation Accuracy: 0.4003, Loss: 2.4235\n",
      "Epoch   8 Batch   40/155 - Train Accuracy: 0.2656, Validation Accuracy: 0.3333, Loss: 3.2946\n",
      "Epoch   8 Batch   50/155 - Train Accuracy: 0.2547, Validation Accuracy: 0.3051, Loss: 3.1239\n",
      "Epoch   8 Batch   60/155 - Train Accuracy: 0.3839, Validation Accuracy: 0.3988, Loss: 3.0723\n",
      "Epoch   8 Batch   70/155 - Train Accuracy: 0.4088, Validation Accuracy: 0.3482, Loss: 2.7269\n",
      "Epoch   8 Batch   80/155 - Train Accuracy: 0.5072, Validation Accuracy: 0.3943, Loss: 2.5360\n",
      "Epoch   8 Batch   90/155 - Train Accuracy: 0.5167, Validation Accuracy: 0.3452, Loss: 2.4434\n",
      "Epoch   8 Batch  100/155 - Train Accuracy: 0.5172, Validation Accuracy: 0.3616, Loss: 2.5096\n",
      "Epoch   8 Batch  110/155 - Train Accuracy: 0.5036, Validation Accuracy: 0.3884, Loss: 2.3579\n",
      "Epoch   8 Batch  120/155 - Train Accuracy: 0.5045, Validation Accuracy: 0.3467, Loss: 2.4035\n",
      "Epoch   8 Batch  130/155 - Train Accuracy: 0.3878, Validation Accuracy: 0.4092, Loss: 3.1784\n",
      "Epoch   8 Batch  140/155 - Train Accuracy: 0.3688, Validation Accuracy: 0.3185, Loss: 3.0762\n",
      "Epoch   8 Batch  150/155 - Train Accuracy: 0.4740, Validation Accuracy: 0.4048, Loss: 2.7562\n",
      "Epoch   9 Batch   10/155 - Train Accuracy: 0.4359, Validation Accuracy: 0.3884, Loss: 2.5705\n",
      "Epoch   9 Batch   20/155 - Train Accuracy: 0.4095, Validation Accuracy: 0.3988, Loss: 2.6961\n",
      "Epoch   9 Batch   30/155 - Train Accuracy: 0.5399, Validation Accuracy: 0.4018, Loss: 2.3534\n",
      "Epoch   9 Batch   40/155 - Train Accuracy: 0.2500, Validation Accuracy: 0.3170, Loss: 3.2083\n",
      "Epoch   9 Batch   50/155 - Train Accuracy: 0.3828, Validation Accuracy: 0.3884, Loss: 3.0338\n",
      "Epoch   9 Batch   60/155 - Train Accuracy: 0.3661, Validation Accuracy: 0.3973, Loss: 3.0078\n",
      "Epoch   9 Batch   70/155 - Train Accuracy: 0.4850, Validation Accuracy: 0.4077, Loss: 2.6379\n",
      "Epoch   9 Batch   80/155 - Train Accuracy: 0.5024, Validation Accuracy: 0.3661, Loss: 2.4837\n",
      "Epoch   9 Batch   90/155 - Train Accuracy: 0.5167, Validation Accuracy: 0.3899, Loss: 2.4044\n",
      "Epoch   9 Batch  100/155 - Train Accuracy: 0.5248, Validation Accuracy: 0.3393, Loss: 2.4650\n",
      "Epoch   9 Batch  110/155 - Train Accuracy: 0.4796, Validation Accuracy: 0.3125, Loss: 2.3228\n",
      "Epoch   9 Batch  120/155 - Train Accuracy: 0.5100, Validation Accuracy: 0.3542, Loss: 2.3351\n",
      "Epoch   9 Batch  130/155 - Train Accuracy: 0.3935, Validation Accuracy: 0.4092, Loss: 3.0772\n",
      "Epoch   9 Batch  140/155 - Train Accuracy: 0.3875, Validation Accuracy: 0.3497, Loss: 2.9818\n",
      "Epoch   9 Batch  150/155 - Train Accuracy: 0.4583, Validation Accuracy: 0.4092, Loss: 2.6824\n",
      "Epoch  10 Batch   10/155 - Train Accuracy: 0.4547, Validation Accuracy: 0.3869, Loss: 2.4716\n",
      "Epoch  10 Batch   20/155 - Train Accuracy: 0.3766, Validation Accuracy: 0.4018, Loss: 2.6169\n",
      "Epoch  10 Batch   30/155 - Train Accuracy: 0.5399, Validation Accuracy: 0.3884, Loss: 2.3003\n",
      "Epoch  10 Batch   40/155 - Train Accuracy: 0.2917, Validation Accuracy: 0.3586, Loss: 3.1204\n",
      "Epoch  10 Batch   50/155 - Train Accuracy: 0.3766, Validation Accuracy: 0.3765, Loss: 2.9439\n",
      "Epoch  10 Batch   60/155 - Train Accuracy: 0.3482, Validation Accuracy: 0.3795, Loss: 2.8653\n",
      "Epoch  10 Batch   70/155 - Train Accuracy: 0.4713, Validation Accuracy: 0.4077, Loss: 2.5869\n",
      "Epoch  10 Batch   80/155 - Train Accuracy: 0.4916, Validation Accuracy: 0.3408, Loss: 2.3885\n",
      "Epoch  10 Batch   90/155 - Train Accuracy: 0.5281, Validation Accuracy: 0.3720, Loss: 2.3371\n",
      "Epoch  10 Batch  100/155 - Train Accuracy: 0.5345, Validation Accuracy: 0.3765, Loss: 2.3591\n",
      "Epoch  10 Batch  110/155 - Train Accuracy: 0.4940, Validation Accuracy: 0.3527, Loss: 2.2390\n",
      "Epoch  10 Batch  120/155 - Train Accuracy: 0.5201, Validation Accuracy: 0.3795, Loss: 2.2769\n",
      "Epoch  10 Batch  130/155 - Train Accuracy: 0.3935, Validation Accuracy: 0.3869, Loss: 3.0718\n",
      "Epoch  10 Batch  140/155 - Train Accuracy: 0.3969, Validation Accuracy: 0.3899, Loss: 2.9276\n",
      "Epoch  10 Batch  150/155 - Train Accuracy: 0.4688, Validation Accuracy: 0.3988, Loss: 2.5915\n",
      "Epoch  11 Batch   10/155 - Train Accuracy: 0.4437, Validation Accuracy: 0.4062, Loss: 2.3599\n",
      "Epoch  11 Batch   20/155 - Train Accuracy: 0.4062, Validation Accuracy: 0.4033, Loss: 2.5455\n",
      "Epoch  11 Batch   30/155 - Train Accuracy: 0.5409, Validation Accuracy: 0.3914, Loss: 2.2338\n",
      "Epoch  11 Batch   40/155 - Train Accuracy: 0.3056, Validation Accuracy: 0.3824, Loss: 2.9450\n",
      "Epoch  11 Batch   50/155 - Train Accuracy: 0.3375, Validation Accuracy: 0.3676, Loss: 2.8622\n",
      "Epoch  11 Batch   60/155 - Train Accuracy: 0.3601, Validation Accuracy: 0.3661, Loss: 2.7806\n",
      "Epoch  11 Batch   70/155 - Train Accuracy: 0.4675, Validation Accuracy: 0.4077, Loss: 2.4997\n",
      "Epoch  11 Batch   80/155 - Train Accuracy: 0.4760, Validation Accuracy: 0.3199, Loss: 2.3068\n",
      "Epoch  11 Batch   90/155 - Train Accuracy: 0.5323, Validation Accuracy: 0.3676, Loss: 2.2740\n",
      "Epoch  11 Batch  100/155 - Train Accuracy: 0.5323, Validation Accuracy: 0.3452, Loss: 2.3157\n",
      "Epoch  11 Batch  110/155 - Train Accuracy: 0.4844, Validation Accuracy: 0.3304, Loss: 2.1984\n",
      "Epoch  11 Batch  120/155 - Train Accuracy: 0.5179, Validation Accuracy: 0.3854, Loss: 2.2554\n",
      "Epoch  11 Batch  130/155 - Train Accuracy: 0.3466, Validation Accuracy: 0.3720, Loss: 2.9196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11 Batch  140/155 - Train Accuracy: 0.4016, Validation Accuracy: 0.3914, Loss: 2.7932\n",
      "Epoch  11 Batch  150/155 - Train Accuracy: 0.4544, Validation Accuracy: 0.3914, Loss: 2.4947\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     source_sequence_length: sources_lengths,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     source_sequence_length: valid_sources_lengths,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence\n",
    "To feed a sentence into the model for translation, you first need to preprocess it.  Implement the function `sentence_to_seq()` to preprocess new sentences.\n",
    "\n",
    "- Convert the sentence to lowercase\n",
    "- Convert words into ids using `vocab_to_int`\n",
    " - Convert words not in the vocabulary, to the `<UNK>` word id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    sent = sentence.lower()\n",
    "    words = sent.split()\n",
    "        \n",
    "    ##2. Convert words into ids \n",
    "    ids = [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in words]\n",
    "    \n",
    "    return list(ids)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate\n",
    "This will translate `translate_sentence` from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = data.abstracts\n",
    "titles = data.titles\n",
    "indx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [15140, 15800, 6243, 5263, 16396, 25116, 666, 7539, 23112, 4459, 19809, 14237, 10819, 6956, 18326, 10819, 17641, 19809, 25116, 22875, 14410, 22208, 15905, 9296, 19809, 12545, 12808, 1719, 15900, 6956, 22933, 19361, 1585, 7620, 5282, 13271, 11051, 15571, 4510, 7539, 23112, 3116, 19671, 9440, 14957, 9296, 8777, 13705, 6243, 498, 7620, 14410, 2850, 7620, 15731, 7539, 23112, 4814, 14957, 6839, 4510, 5735, 1727, 18279, 12215, 19809, 821, 7539, 23112, 18431, 4510, 14061, 14410, 8449, 8599, 16831, 20510, 6202, 3510, 4510, 19809, 821, 6583, 9296, 19809, 8090, 1036, 9645, 6354, 24389, 21393, 19809, 8637, 10921, 17173, 23112, 6956, 3675, 12767, 12769, 15140, 8516, 14410, 17097, 17747, 666, 19809, 7539, 23112, 3116, 11023, 152, 14410, 4814, 7620, 12545, 14957, 7831, 17104, 4996, 5927, 24532, 3510, 4510, 19809, 8090, 9645, 6354, 10020, 19809, 468, 9645, 9201, 9296, 19809, 19809, 12476, 3542, 3906, 6073, 666, 16335, 1841, 24163, 4311, 18678, 13271, 17980, 14410, 19348, 6956, 1357, 7054, 17637, 17104, 2965, 17672, 20142, 22851, 19567, 24247, 15325, 4510, 21877, 6583, 19809, 12210, 7620, 1841, 6956, 15731, 24163, 7539, 23112, 12370, 24430, 21886, 21393, 16472, 14410, 19158, 16060, 666, 19809, 7307, 8374, 24588]\n",
      "  English Words: ['we', 'present', 'an', 'ultra', 'deep', 'survey', 'for', 'neptune', 'trojans', 'using', 'the', 'subaru', 'm', 'and', 'magellan', 'm', 'telescopes', 'the', 'survey', 'reached', 'a', 'detection', 'efficiency', 'in', 'the', 'r', 'band', 'at', 'magnitudes', 'and', 'covered', 'square', 'degrees', 'of', 'sky', 'this', 'depth', 'corresponds', 'to', 'neptune', 'trojans', 'that', 'are', 'about', 'km', 'in', 'radius', 'assuming', 'an', 'albedo', 'of', 'a', 'paucity', 'of', 'smaller', 'neptune', 'trojans', 'radii', 'km', 'compared', 'to', 'larger', 'ones', 'was', 'found', 'the', 'brightest', 'neptune', 'trojans', 'appear', 'to', 'follow', 'a', 'steep', 'power', 'law', 'slope', 'q', 'similar', 'to', 'the', 'brightest', 'objects', 'in', 'the', 'other', 'known', 'stable', 'reservoirs', 'such', 'as', 'the', 'kuiper', 'belt,', 'jupiter', 'trojans', 'and', 'main', 'belt', 'asteroids', 'we', 'find', 'a', 'roll', 'over', 'for', 'the', 'neptune', 'trojans', 'that', 'occurs', 'around', 'a', 'radii', 'of', 'r', 'km', 'mags,', 'which', 'is', 'also', 'very', 'similar', 'to', 'the', 'other', 'stable', 'reservoirs', 'all', 'the', 'observed', 'stable', 'regions', 'in', 'the', 'the', 'solar', 'system', 'show', 'evidence', 'for', 'missing', 'intermediate', 'sized', 'planetesimals', 'misps', 'this', 'indicates', 'a', 'primordial', 'and', 'not', 'collisional', 'origin,', 'which', 'suggests', 'planetesimal', 'formation', 'proceeded', 'directly', 'from', 'small', 'to', 'large', 'objects', 'the', 'scarcity', 'of', 'intermediate', 'and', 'smaller', 'sized', 'neptune', 'trojans', 'may', 'limit', 'them', 'as', 'being', 'a', 'strong', 'source', 'for', 'the', 'short', 'period', 'comets']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [544, 4862, 3175, 1204, 544, 54, 4236, 1]\n",
      "  French Words: the california search of the hot jupiter <EOS>\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = abstracts[0]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
